{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAQ VIGIE - METHODOLOGY\n",
    "\n",
    "### Loading Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# ML Libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Global Parameters\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Twitter's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import webbrowser\n",
    "import time\n",
    "\n",
    "consumer_key = \"Qt7XQIJW9Ey19v8w3QtSELMTb\"\n",
    "consumer_secret_key = \"tXdjubhBT0QUrsTIaF9SzTptKxPE9TZohruePiGvlrydwDnebx\"\n",
    "\n",
    "callback_url = 'oob' \n",
    "\n",
    "# Authenticates the application\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret_key, callback_url)\n",
    "\n",
    "# Grab a URL that we can send our user to\n",
    "redirect_url = auth.get_authorization_url()\n",
    "\n",
    "# Test the Twitter API Application URL \n",
    "print(redirect_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webbrowser.open(redirect_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PIN form in order to authenticate the URL even if the PIN value changes\n",
    "user_pin_input = input(\"What's the PIN value? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pin_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access keys for the particular user. These don't change.\n",
    "auth.get_access_token(user_pin_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the API with my own Twitter Profile\n",
    "me = api.me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing my username\n",
    "print(me.screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAQ\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,q=\"@LaSAQ_officiel\",count=20000000,since=\"2022-06-01\", wait_on_rate_limit=True).items()\n",
    "all_tweets = [[tweet.text, tweet.created_at, tweet.text, tweet.user, tweet.user.location] for tweet in tweets]\n",
    "\n",
    "SAQ_1 = pd.DataFrame(data=all_tweets, columns=['tweet','created at','text','user', \"location\"])\n",
    "SAQ_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAQ\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,q=\"SAQ\",count=20000000, since=\"2022-06-01\",wait_on_rate_limit=True).items()\n",
    "all_tweets = [[tweet.text, tweet.created_at, tweet.text, tweet.user, tweet.user.location] for tweet in tweets]\n",
    "\n",
    "SAQ_2 = pd.DataFrame(data=all_tweets, columns=['tweet','created at','text','user', \"location\"])\n",
    "SAQ_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOCIÉTÉ DES ALCOOLS DU QUÉBEC\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,q=\"Société des alcools du Québec\",count=20000000, since=\"2022-06-01\",wait_on_rate_limit=True).items()\n",
    "all_tweets = [[tweet.text, tweet.created_at, tweet.text, tweet.user, tweet.user.location] for tweet in tweets]\n",
    "\n",
    "SAQ_3 = pd.DataFrame(data=all_tweets, columns=['tweet','created at','text','user', \"location\"])\n",
    "SAQ_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the three different outputs from the API\n",
    "\n",
    "frames = [SAQ_1, SAQ_2, SAQ_3]\n",
    "  \n",
    "result = pd.concat(frames)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and separating hashtags, links, mentions\n",
    "\n",
    "import re \n",
    "\n",
    "def remove_punct(text):\n",
    "    text = re.sub(r\"http\\S+\",' ', text)\n",
    "    text = re.sub(r'\\B@\\w*[a-zA-Z]+\\w*',' ', text)\n",
    "    text = re.sub(r'RT','', text)\n",
    "    text = re.sub(r':','', text)\n",
    "    text = re.sub(r'\\B#\\w*[a-zA-Z]+\\w*',' ', text)\n",
    "    text = re.sub(r\"@(\\w+)\", ' ',text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "result['tweet'] = result['tweet'].apply(str)\n",
    "result['Tweet_Clean'] = result['tweet'].apply(lambda x: remove_punct(x))\n",
    "result['Mentions'] = result['tweet'].apply(lambda x: re.findall(r'\\B@\\w*[a-zA-Z]+\\w*', x))\n",
    "result['hashtag'] = result['tweet'].apply(lambda x: re.findall(r'\\B#\\w*[a-zA-Z]+\\w*', x))\n",
    "\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language detector to separate EN/FR\n",
    "\n",
    "langue = []\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "for i in result[\"tweet\"]:\n",
    "    try:\n",
    "        a = detect(i)\n",
    "    except:\n",
    "        language = \"error\"\n",
    "        print(\"This row throws and error:\", i[0])\n",
    "    langue.append(a)\n",
    "\n",
    "result[\"langue\"]=langue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating EN/FR\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# DF FRANCAIS\n",
    "fr = result[(result.langue == \"fr\")]\n",
    "\n",
    "# DF ANGLAIS\n",
    "en = result[(result.langue == \"en\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSE SENTIMENT - FRANCAIS\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "senti_list = []\n",
    "for i in fr[\"tweet\"]:\n",
    "    vs = tb(i).sentiment[0]\n",
    "    if (vs > 0):\n",
    "        senti_list.append('Positive')\n",
    "    elif (vs < 0):\n",
    "        senti_list.append('Negative')\n",
    "    else:\n",
    "        senti_list.append('Neutral')   \n",
    "\n",
    "polarity = []\n",
    "for i in fr[\"tweet\"]:\n",
    "    vs = tb(i).sentiment[0]\n",
    "    polarity.append(vs)\n",
    "\n",
    "fr[\"polarity\"]=polarity    \n",
    "fr[\"sentiment\"]=senti_list\n",
    "fr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSE SENTIMENT - ANGLAIS\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "from textblob import Blobber\n",
    "tb = Blobber()\n",
    "\n",
    "senti_list_en = []\n",
    "for i in en[\"tweet\"]:\n",
    "    vs = tb(i).sentiment[0]\n",
    "    if (vs > 0):\n",
    "        senti_list_en.append('Positive')\n",
    "    elif (vs < 0):\n",
    "        senti_list_en.append('Negative')\n",
    "    else:\n",
    "        senti_list_en.append('Neutral')   \n",
    "\n",
    "polarity_en = []\n",
    "for i in en[\"tweet\"]:\n",
    "    vs = tb(i).sentiment[0]\n",
    "    polarity_en.append(vs)\n",
    "\n",
    "en[\"polarity\"]=polarity_en   \n",
    "en[\"sentiment\"]=senti_list_en\n",
    "en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate WordCloud\n",
    " \n",
    "# importing all necessary modules\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    " \n",
    "df = fb\n",
    " \n",
    "comment_words = ''\n",
    "stopwords = [\"pendant\",\"qu\",\"michel  béland\",\"patrick  ferland\",\"jerome lagrange\",\"michel béland\",\"aux\",\"patrice gravel\",\"nan\",\"nan nan\",\"voir\",\"plus\",\"c\",\"n\",\"l\",\"tony\",\"maxime\",\"d\",\"m\",\"aux,\"\"vers\",\"richard\",\"mets\",\"n'ai\",\"andré\",\"allons\",\"qu'il\",\"d'autres\",\"faut\",\"qu'ils\",\"ont\",\"s'ils\",\"main\",\"n'ont\",\"ca\",\"n'est\",\"un\",\"leurs\",\"ne\",\"une\",\"c'est\",\"s'est\",\"C'est\",\"telle\",\"se\",\"temp\",\"va\",\"y\",\"je\",\"pat77432407\",\"à\",\"j'ai\",\"lasaq_officiel\",\"de\",\"rt\",\"saq\",\"https\",\"co\",\"alors\",\"au\",\"aucuns\",\"aussi\",\"autre\",\"avant\",\"avec\",\"avoir\",\"bon\",\"car\",\"ce\",\"cela\",\"ces\",\"ceux\",\"chaque\",\"ci\",\"comme\",\"comment\",\"dans\",\"des\",\"du\",\"dedans\",\"dehors\",\"depuis\",\"devrait\",\"doit\",\"donc\",\"dos\",\"début\",\"elle\",\"elles\",\"en\",\"encore\",\"essai\",\"est\",\"et\",\"eu\",\"fait\",\"faites\",\"fois\",\"font\",\"hors\",\"ici\",\"il\",\"ils\",\"je\tjuste\",\"la\",\"le\",\"les\",\"leur\",\"là\",\"ma\",\"maintenant\",\"mais\",\"mes\",\"mien\",\"moins\",\"mon\",\"mot\",\"même\",\"ni\",\"nommés\",\"notre\",\"nous\",\"ou\",\"où\",\"par\",\"parce\",\"pas\",\"peut\",\"peu\",\"plupart\",\"pour\",\"pourquoi\",\"quand\",\"que\",\"quel\",\"quelle\",\"quelles\",\"quels\",\"qui\",\"sa\",\"sans\",\"ses\",\"seulement\",\"si\",\"sien\",\"son\",\"sont\",\"sous\",\"soyez\tsujet\",\"sur\",\"ta\",\"tandis\",\"tellement\",\"tels\",\"tes\",\"ton\",\"tous\",\"tout\",\"trop\",\"très\",\"tu\",\"voient\",\"vont\",\"votre\",\"vous\",\"vu\",\"ça\",\"étaient\",\"état\",\"étions\",\"été\",\"être\",\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"]\n",
    " \n",
    "# iterate through the csv file\n",
    "for val in df.Commentaire:\n",
    "     \n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    " \n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "     \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "wordcloud = WordCloud(width = 800, \n",
    "                height = 800, \n",
    "                max_words = 30,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                font_path='Arial',\n",
    "                colormap='Reds').generate(comment_words)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
